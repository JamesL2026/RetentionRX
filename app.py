import streamlit as st
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report
import lightgbm as lgb
import plotly.express as px
import plotly.graph_objects as go
import json
import os
from datetime import datetime
import openai
from io import StringIO

# Page config
st.set_page_config(
    page_title="RetentionRx - Customer Churn Prediction & Playbook Generator",
    page_icon="ðŸ”®",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .playbook-section {
        background-color: #f8f9fa;
        padding: 1.5rem;
        border-radius: 0.5rem;
        border: 1px solid #e9ecef;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'model_trained' not in st.session_state:
    st.session_state.model_trained = False
if 'model' not in st.session_state:
    st.session_state.model = None
if 'feature_importance' not in st.session_state:
    st.session_state.feature_importance = None
if 'auc_score' not in st.session_state:
    st.session_state.auc_score = None
if 'df_with_predictions' not in st.session_state:
    st.session_state.df_with_predictions = None
if 'column_mapping' not in st.session_state:
    st.session_state.column_mapping = None
if 'saved_datasets' not in st.session_state:
    st.session_state.saved_datasets = {}
if 'current_dataset_name' not in st.session_state:
    st.session_state.current_dataset_name = None
if 'current_df' not in st.session_state:
    st.session_state.current_df = None

def load_sample_data():
    """Load the sample CSV data"""
    return pd.read_csv('sample_data.csv')

def save_dataset_to_library(df, dataset_name, column_mapping):
    """Save dataset to the library"""
    st.session_state.saved_datasets[dataset_name] = {
        'data': df.copy(),
        'column_mapping': column_mapping.copy(),
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }

def load_dataset_from_library(dataset_name):
    """Load dataset from the library"""
    if dataset_name in st.session_state.saved_datasets:
        dataset = st.session_state.saved_datasets[dataset_name]
        return dataset['data'].copy(), dataset['column_mapping'].copy()
    return None, None

def reset_model_state():
    """Reset all model-related state when switching datasets"""
    st.session_state.model_trained = False
    st.session_state.model = None
    st.session_state.feature_importance = None
    st.session_state.auc_score = None
    st.session_state.df_with_predictions = None

def calculate_rfm_analysis(df):
    """Calculate RFM (Recency, Frequency, Monetary) analysis"""
    rfm_df = df.copy()
    
    # Calculate RFM metrics
    rfm_df['Recency'] = df['last_purchase_days_ago']
    rfm_df['Frequency'] = np.maximum(1, df['tenure_days'] / 30)  # Approximate monthly frequency
    rfm_df['Monetary'] = df['monthly_spend']
    
    # Create RFM scores (1-5 scale)
    rfm_df['R_Score'] = pd.qcut(rfm_df['Recency'], 5, labels=[5,4,3,2,1])  # Inverted for recency
    rfm_df['F_Score'] = pd.qcut(rfm_df['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])
    rfm_df['M_Score'] = pd.qcut(rfm_df['Monetary'], 5, labels=[1,2,3,4,5])
    
    # Create RFM segment
    rfm_df['RFM_Segment'] = rfm_df['R_Score'].astype(str) + rfm_df['F_Score'].astype(str) + rfm_df['M_Score'].astype(str)
    
    # Define customer segments
    segment_map = {
        '555': 'Champions', '554': 'Champions', '544': 'Champions', '545': 'Champions',
        '543': 'Loyal Customers', '444': 'Loyal Customers', '435': 'Loyal Customers',
        '234': 'Potential Loyalists', '233': 'Potential Loyalists', '232': 'Potential Loyalists',
        '111': 'New Customers', '112': 'New Customers', '121': 'New Customers',
        '521': 'At Risk', '522': 'At Risk', '523': 'At Risk', '524': 'At Risk',
        '421': 'Cannot Lose Them', '422': 'Cannot Lose Them', '423': 'Cannot Lose Them',
        '321': 'Need Attention', '322': 'Need Attention', '323': 'Need Attention'
    }
    
    rfm_df['Customer_Segment'] = rfm_df['RFM_Segment'].map(segment_map).fillna('Others')
    return rfm_df

def calculate_clv_metrics(df):
    """Calculate Customer Lifetime Value metrics"""
    clv_df = df.copy()
    
    # Current CLV: Monthly Spend Ã— (Tenure in months)
    clv_df['Current_CLV'] = clv_df['monthly_spend'] * (clv_df['tenure_days'] / 30)
    
    # Predicted CLV: Current CLV Ã— (1 - Churn Probability)
    if 'churn_probability' in clv_df.columns:
        clv_df['Predicted_CLV'] = clv_df['Current_CLV'] * (1 - clv_df['churn_probability'])
        clv_df['CLV_at_Risk'] = clv_df['Current_CLV'] * clv_df['churn_probability']
    else:
        clv_df['Predicted_CLV'] = clv_df['Current_CLV']
        clv_df['CLV_at_Risk'] = clv_df['Current_CLV'] * 0.1  # Assume 10% risk if no model
    
    return clv_df

def calculate_advanced_metrics(df):
    """Calculate advanced customer metrics"""
    metrics_df = df.copy()
    
    # Customer Health Score (0-100)
    health_score = (
        (1 - metrics_df['last_purchase_days_ago'] / 365) * 25 +  # Recency (25%)
        (metrics_df['avg_nps'] / 10) * 25 +  # Satisfaction (25%)
        (metrics_df['monthly_spend'] / metrics_df['monthly_spend'].max()) * 25 +  # Spend (25%)
        (1 - metrics_df['num_tickets_30d'] / 10) * 25  # Low ticket count (25%)
    ) * 100
    
    metrics_df['Health_Score'] = np.clip(health_score, 0, 100)
    
    # Risk Score (0-100, higher = more risky)
    risk_score = (
        (metrics_df['num_tickets_30d'] / 10) * 30 +  # High ticket count (30%)
        (metrics_df['last_purchase_days_ago'] / 365) * 25 +  # Inactivity (25%)
        (1 - metrics_df['avg_nps'] / 10) * 25 +  # Low satisfaction (25%)
        (1 - metrics_df['monthly_spend'] / metrics_df['monthly_spend'].max()) * 20  # Low spend (20%)
    ) * 100
    
    metrics_df['Risk_Score'] = np.clip(risk_score, 0, 100)
    
    return metrics_df

def validate_csv_schema(df):
    """Validate that CSV has required columns or suggest mappings"""
    # Required columns with common variations
    required_mappings = {
        'customer_id': ['customer_id', 'id', 'customer', 'user_id', 'client_id', 'account_id'],
        'tenure_days': ['tenure_days', 'tenure', 'days_since_signup', 'customer_age_days', 'account_age'],
        'monthly_spend': ['monthly_spend', 'spend', 'revenue', 'monthly_revenue', 'avg_spend', 'spending'],
        'num_tickets_30d': ['num_tickets_30d', 'tickets', 'support_tickets', 'tickets_30d', 'support_calls'],
        'avg_nps': ['avg_nps', 'nps', 'satisfaction', 'customer_satisfaction', 'satisfaction_score'],
        'last_purchase_days_ago': ['last_purchase_days_ago', 'days_since_purchase', 'last_order', 'days_since_order'],
        'churn': ['churn', 'churned', 'is_churned', 'churn_flag', 'retained', 'status']
    }
    
    found_columns = {}
    missing_columns = []
    
    for required, variations in required_mappings.items():
        found = False
        for variation in variations:
            if variation in df.columns:
                found_columns[required] = variation
                found = True
                break
        if not found:
            missing_columns.append(required)
    
    return missing_columns, found_columns

def train_churn_model(df, column_mapping=None):
    """Train a churn prediction model"""
    if column_mapping is None:
        # Use default column names
        feature_columns = ['tenure_days', 'monthly_spend', 'num_tickets_30d', 
                          'avg_nps', 'last_purchase_days_ago']
        X = df[feature_columns]
        y = df['churn']
    else:
        # Use mapped column names
        feature_columns = [column_mapping['tenure_days'], column_mapping['monthly_spend'], 
                          column_mapping['num_tickets_30d'], column_mapping['avg_nps'], 
                          column_mapping['last_purchase_days_ago']]
        X = df[feature_columns]
        y = df[column_mapping['churn']]
    
    # Rename columns to standard names for consistency
    X.columns = ['tenure_days', 'monthly_spend', 'num_tickets_30d', 'avg_nps', 'last_purchase_days_ago']
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # Train LightGBM model
    train_data = lgb.Dataset(X_train, label=y_train)
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'verbose': -1
    }
    
    model = lgb.train(params, train_data, num_boost_round=100, valid_sets=[train_data], callbacks=[lgb.log_evaluation(0)])
    
    # Make predictions
    y_pred_proba = model.predict(X_test, num_iteration=model.best_iteration)
    auc_score = roc_auc_score(y_test, y_pred_proba)
    
    # Get feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_columns,
        'importance': model.feature_importance(importance_type='gain')
    }).sort_values('importance', ascending=False)
    
    # Add predictions to full dataset
    df_with_predictions = df.copy()
    all_predictions = model.predict(df[feature_columns], num_iteration=model.best_iteration)
    df_with_predictions['churn_probability'] = all_predictions
    
    return model, feature_importance, auc_score, df_with_predictions

def generate_playbook_llm(segment_summary, top_drivers, api_key=None):
    """Generate playbook using OpenAI API"""
    if not api_key:
        return None
    
    try:
        openai.api_key = api_key
        
        system_prompt = """You are an experienced enterprise CX consultant. Produce a concise JSON playbook."""
        
        user_prompt = f"""Given this segment summary and top 3 drivers, output valid JSON with these keys:
- priority_actions: array of short action strings (max 5)
- agent_script: short string (200 chars)
- ops_fixes: array of operational changes (max 4)
- kpis_to_track: array of KPI strings
- assumptions: array of assumptions

Segment summary: {segment_summary}
Top drivers: {top_drivers}

Constraints: low budget ($10k/month), timeline 90 days.
Return only valid JSON."""

        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.2,
            max_tokens=400
        )
        
        playbook_text = response.choices[0].message.content.strip()
        
        # Try to parse as JSON
        try:
            playbook = json.loads(playbook_text)
            return playbook
        except json.JSONDecodeError:
            # If JSON parsing fails, return the raw text
            return {"raw_text": playbook_text}
            
    except Exception as e:
        st.error(f"Error calling OpenAI API: {str(e)}")
        return None

def get_canned_playbook():
    """Return canned playbook as fallback"""
    return {
        "priority_actions": [
            "Outbound 2-minute check-in call to top 50 customers in the segment (within 48 hours)",
            "Auto-create high-priority ticket for customers with 3+ tickets in 30 days and assign to senior agent",
            "Offer targeted 20% promo or expedited shipping for customers with recent complaints"
        ],
        "agent_script": "Hi [Name], I'm [Agent]. I see you had issues recently â€” I'd like to resolve this now and offer a tailored solution. Can I confirm a quick detail?",
        "ops_fixes": [
            "Reduce support SLA for top-risk segment to 24 hours",
            "Flag accounts with 2+ tickets for supervisor review",
            "Add 'recovery offer' workflow in the CRM for these customers"
        ],
        "kpis_to_track": [
            "Churn rate for pilot group (30/60/90 days)",
            "Average resolution time for flagged tickets",
            "Customer satisfaction/NPS post-intervention"
        ],
        "assumptions": [
            "Pilot targets top 100 high-risk customers",
            "Average revenue/customer = $100/month",
            "Expected conservative churn reduction = 10%"
        ]
    }

def main():
    # Header
    st.markdown('<h1 class="main-header">ðŸ”® RetentionRx</h1>', unsafe_allow_html=True)
    st.markdown("**Find customers likely to churn, explain why, and generate actionable rescue plans**")
    
    # Quick explanation
    with st.expander("ðŸ“– How RetentionRx Works", expanded=False):
        st.markdown("""
        **RetentionRx helps you identify and save at-risk customers in 3 simple steps:**
        
        1. **ðŸŽ¯ Predict Risk** - AI model analyzes customer data to predict churn probability
        2. **ðŸ” Explain Why** - Shows which factors (tickets, satisfaction, spending) drive risk
        3. **ðŸ“‹ Take Action** - Generates specific rescue plans with scripts and ROI estimates
        
        **Perfect for:** Customer success teams, retention managers, and CX leaders who want to move from insight to action quickly.
        """)
    
    # Main Navigation Tabs
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["ðŸŽ¯ Churn Prediction", "ðŸ“Š Customer Analytics", "ðŸ’° Revenue Insights", "ðŸ” Flexible Analytics", "ðŸ“š Glossary", "ðŸš€ Advanced Features"])
    
    with tab1:
        churn_prediction_tab()
    
    with tab2:
        customer_analytics_tab()
    
    with tab3:
        revenue_insights_tab()
    
    with tab4:
        flexible_analytics_tab()
    
    with tab5:
        glossary_tab()
    
    with tab6:
        advanced_features_tab()

def churn_prediction_tab():
    """Original churn prediction functionality"""
    
    # Sidebar
    st.sidebar.title("Configuration")
    
    # Data upload
    st.sidebar.subheader("ðŸ“Š Data Input")
    
    # Dataset library
    st.sidebar.subheader("ðŸ“š Dataset Library")
    
    # Show saved datasets
    if st.session_state.saved_datasets:
        st.sidebar.markdown("**Saved Datasets:**")
        for name, dataset in st.session_state.saved_datasets.items():
            col1, col2 = st.sidebar.columns([3, 1])
            with col1:
                if st.sidebar.button(f"ðŸ“ {name}", key=f"load_{name}", use_container_width=True):
                    df, column_mapping = load_dataset_from_library(name)
                    st.session_state.current_df = df
                    st.session_state.column_mapping = column_mapping
                    st.session_state.current_dataset_name = name
                    reset_model_state()
                    st.rerun()
            with col2:
                if st.sidebar.button("ðŸ—‘ï¸", key=f"delete_{name}", help="Delete dataset"):
                    del st.session_state.saved_datasets[name]
                    if st.session_state.current_dataset_name == name:
                        st.session_state.current_dataset_name = None
                        st.session_state.current_df = None
                        reset_model_state()
                    st.rerun()
            
            st.sidebar.caption(f"ðŸ“… {dataset['timestamp']}")
    else:
        st.sidebar.info("No saved datasets yet")
    
    st.sidebar.markdown("---")
    
    # Data source selection
    upload_option = st.sidebar.radio(
        "Choose data source:",
        ["Use Sample Data", "Upload New CSV"]
    )
    
    df = None
    column_mapping = None
    
    if upload_option == "Use Sample Data":
        df = load_sample_data()
        st.sidebar.success("âœ… Sample data loaded")
        st.session_state.current_dataset_name = "Sample Data"
        st.session_state.current_df = df
    else:
        uploaded_file = st.sidebar.file_uploader("Upload CSV", type="csv")
        if uploaded_file is not None:
            df = pd.read_csv(uploaded_file)
            st.sidebar.success("âœ… CSV uploaded")
            st.session_state.current_dataset_name = uploaded_file.name
            st.session_state.current_df = df
    
    # Use current dataset if available, otherwise use uploaded/selected data
    if st.session_state.current_df is not None:
        df = st.session_state.current_df
        column_mapping = st.session_state.column_mapping
    
    if df is not None:
        # Validate schema and get column mappings
        missing_columns, column_mapping = validate_csv_schema(df)
        
        if missing_columns:
            st.error(f"âŒ Missing required columns: {missing_columns}")
            
            # Show column mapping suggestions
            st.subheader("ðŸ” Column Mapping Suggestions")
            st.markdown("**Your CSV columns:**")
            st.write(df.columns.tolist())
            
            st.markdown("**Required columns with common variations:**")
            suggestions = {
                'Customer ID': ['customer_id', 'id', 'customer', 'user_id', 'client_id', 'account_id'],
                'Tenure (days)': ['tenure_days', 'tenure', 'days_since_signup', 'customer_age_days', 'account_age'],
                'Monthly Spend': ['monthly_spend', 'spend', 'revenue', 'monthly_revenue', 'avg_spend', 'spending'],
                'Support Tickets': ['num_tickets_30d', 'tickets', 'support_tickets', 'tickets_30d', 'support_calls'],
                'Customer Satisfaction': ['avg_nps', 'nps', 'satisfaction', 'customer_satisfaction', 'satisfaction_score'],
                'Last Purchase': ['last_purchase_days_ago', 'days_since_purchase', 'last_order', 'days_since_order'],
                'Churn Label': ['churn', 'churned', 'is_churned', 'churn_flag', 'retained', 'status']
            }
            
            for req_col, variations in suggestions.items():
                st.write(f"**{req_col}:** {', '.join(variations)}")
            
            st.info("ðŸ’¡ **Tip:** Rename your CSV columns to match any of the suggested variations, or use the sample data to test the app.")
            st.stop()
        
        # Show current dataset info
        if st.session_state.current_dataset_name:
            st.info(f"ðŸ“Š **Current Dataset:** {st.session_state.current_dataset_name}")
            
            # Warning if model was trained on different data
            if st.session_state.model_trained:
                st.warning("âš ï¸ **Model trained on this dataset.** Switch datasets will reset the model.")
                
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("ðŸ”„ Switch Dataset & Reset Model", type="secondary"):
                        reset_model_state()
                        st.rerun()
                with col2:
                    if st.button("âœ… Keep Current Dataset", type="primary"):
                        pass
        
        # Show successful column mapping and save option
        if column_mapping:
            st.success("âœ… Column mapping successful!")
            st.markdown("**Detected columns:**")
            for standard, detected in column_mapping.items():
                st.write(f"â€¢ {standard}: `{detected}`")
            
            # Save dataset to library
            col1, col2 = st.columns([2, 1])
            with col1:
                dataset_name = st.text_input(
                    "Save dataset as:", 
                    value=st.session_state.current_dataset_name or "My Dataset",
                    help="Give your dataset a memorable name"
                )
            with col2:
                if st.button("ðŸ’¾ Save to Library", type="secondary"):
                    if dataset_name and dataset_name.strip():
                        save_dataset_to_library(df, dataset_name.strip(), column_mapping)
                        st.success(f"âœ… Dataset '{dataset_name}' saved to library!")
                        st.rerun()
                    else:
                        st.error("Please enter a dataset name")
        
        # Data preview
        st.subheader("ðŸ“‹ Data Preview")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total Customers", len(df))
        with col2:
            # Use mapped column name for churn
            churn_col = column_mapping['churn'] if column_mapping else 'churn'
            st.metric("Churn Rate", f"{df[churn_col].mean():.1%}")
        with col3:
            # Use mapped column name for monthly spend
            spend_col = column_mapping['monthly_spend'] if column_mapping else 'monthly_spend'
            st.metric("Avg Monthly Spend", f"${df[spend_col].mean():.2f}")
        
        st.dataframe(df.head(10), use_container_width=True)
        
        # Model training
        st.subheader("ðŸ¤– Model Training")
        if st.button("Train Churn Prediction Model", type="primary"):
            with st.spinner("Training model..."):
                model, feature_importance, auc_score, df_with_predictions = train_churn_model(df, column_mapping)
                
                st.session_state.model = model
                st.session_state.feature_importance = feature_importance
                st.session_state.auc_score = auc_score
                st.session_state.df_with_predictions = df_with_predictions
                st.session_state.column_mapping = column_mapping
                st.session_state.model_trained = True
                
                st.success(f"âœ… Model trained! AUC Score: {auc_score:.3f}")
        
        if st.session_state.model_trained:
            # Model performance
            st.subheader("ðŸ“ˆ Model Performance")
            col1, col2 = st.columns(2)
            
            with col1:
                auc_score = st.session_state.auc_score
                st.metric("Model Accuracy", f"{auc_score:.1%}")
                
                # Explain AUC score in simple terms
                if auc_score >= 0.9:
                    st.success("ðŸŽ¯ Excellent - Model is very reliable")
                elif auc_score >= 0.8:
                    st.success("âœ… Good - Model is reliable for predictions")
                elif auc_score >= 0.7:
                    st.warning("âš ï¸ Fair - Model is somewhat reliable")
                else:
                    st.error("âŒ Poor - Model needs improvement")
                
                # Simple explanation
                st.info(f"**What this means:** {auc_score:.1%} accuracy means the model correctly identifies churn risk {auc_score:.1%} of the time. Higher is better!")
            
            with col2:
                # Feature importance chart with explanations
                st.markdown("**ðŸ” What Drives Churn Risk:**")
                
                # Add human-readable feature names
                feature_names = {
                    'num_tickets_30d': 'Support Tickets (30 days)',
                    'avg_nps': 'Customer Satisfaction Score',
                    'last_purchase_days_ago': 'Days Since Last Purchase',
                    'tenure_days': 'Customer Tenure (days)',
                    'monthly_spend': 'Monthly Spending'
                }
                
                df_display = st.session_state.feature_importance.copy()
                df_display['feature_display'] = df_display['feature'].map(feature_names)
                
                fig = px.bar(
                    df_display,
                    x='importance',
                    y='feature_display',
                    orientation='h',
                    title="Top Churn Risk Factors",
                    color='importance',
                    color_continuous_scale='Reds'
                )
                fig.update_layout(height=300, yaxis_title="Risk Factors")
                st.plotly_chart(fig, use_container_width=True)
                
                # Explain the top factor
                top_factor = df_display.iloc[0]['feature']
                top_factor_name = feature_names[top_factor]
                st.info(f"ðŸ’¡ **Key Insight:** {top_factor_name} is the strongest predictor of churn risk")
            
            # Risk segmentation
            st.subheader("ðŸŽ¯ Risk Segmentation")
            
            st.markdown("**How it works:** Customers with churn probability above the threshold are flagged as high-risk and need immediate attention.")
            
            # Threshold slider with better explanation
            threshold = st.slider(
                "Risk Threshold",
                min_value=0.1,
                max_value=0.9,
                value=0.5,
                step=0.05,
                help="Customers above this threshold are considered high-risk and need immediate attention"
            )
            
            # Explain threshold in simple terms
            if threshold >= 0.7:
                st.warning("âš ï¸ **High threshold** - Only the most at-risk customers will be flagged")
            elif threshold >= 0.5:
                st.info("ðŸŽ¯ **Balanced threshold** - Moderate risk customers will be included")
            else:
                st.success("ðŸ“¢ **Low threshold** - Many customers will be flagged for proactive outreach")
            
            # Filter high-risk customers
            high_risk_customers = st.session_state.df_with_predictions[
                st.session_state.df_with_predictions['churn_probability'] > threshold
            ].sort_values('churn_probability', ascending=False)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ðŸš¨ High-Risk Customers", len(high_risk_customers))
            with col2:
                if len(high_risk_customers) > 0:
                    avg_spend = high_risk_customers['monthly_spend'].mean()
                    st.metric("ðŸ’° Revenue at Risk", f"${avg_spend * len(high_risk_customers):.0f}/month")
            with col3:
                if len(high_risk_customers) > 0:
                    avg_risk = high_risk_customers['churn_probability'].mean()
                    st.metric("âš¡ Avg Risk Level", f"{avg_risk:.0%}")
            
            # Top risky customers table
            if len(high_risk_customers) > 0:
                st.subheader("ðŸš¨ Top High-Risk Customers")
                display_cols = ['customer_id', 'churn_probability', 'monthly_spend', 
                              'tenure_days', 'num_tickets_30d', 'avg_nps']
                st.dataframe(
                    high_risk_customers[display_cols].head(20),
                    use_container_width=True
                )
                
                # Generate playbook section
                st.subheader("ðŸ“‹ Generate Rescue Playbook")
                
                # OpenAI API key input
                openai_key = st.text_input(
                    "OpenAI API Key (optional)",
                    type="password",
                    help="Leave empty to use canned playbook"
                )
                
                if st.button("Generate Playbook", type="primary"):
                    # Create segment summary
                    segment_summary = f"""
                    High-risk customer segment with {len(high_risk_customers)} customers.
                    Average churn probability: {high_risk_customers['churn_probability'].mean():.2f}
                    Average monthly spend: ${high_risk_customers['monthly_spend'].mean():.2f}
                    Average tenure: {high_risk_customers['tenure_days'].mean():.0f} days
                    Average tickets (30d): {high_risk_customers['num_tickets_30d'].mean():.1f}
                    Average NPS: {high_risk_customers['avg_nps'].mean():.1f}
                    """
                    
                    # Get top drivers
                    top_drivers = st.session_state.feature_importance.head(3)['feature'].tolist()
                    top_drivers_text = ", ".join(top_drivers)
                    
                    # Generate playbook
                    playbook = None
                    if openai_key:
                        with st.spinner("Generating playbook with AI..."):
                            playbook = generate_playbook_llm(segment_summary, top_drivers_text, openai_key)
                    
                    if playbook is None or "raw_text" in playbook:
                        st.info("Using canned playbook (API unavailable or invalid)")
                        playbook = get_canned_playbook()
                    
                    # Display playbook
                    st.markdown('<div class="playbook-section">', unsafe_allow_html=True)
                    st.subheader("ðŸŽ¯ Generated Rescue Playbook")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**Priority Actions:**")
                        for i, action in enumerate(playbook.get('priority_actions', []), 1):
                            st.write(f"{i}. {action}")
                        
                        st.markdown("**Agent Script:**")
                        st.write(f'"{playbook.get("agent_script", "")}"')
                    
                    with col2:
                        st.markdown("**Operations Fixes:**")
                        for i, fix in enumerate(playbook.get('ops_fixes', []), 1):
                            st.write(f"{i}. {fix}")
                        
                        st.markdown("**KPIs to Track:**")
                        for kpi in playbook.get('kpis_to_track', []):
                            st.write(f"â€¢ {kpi}")
                    
                    # Assumptions and ROI
                    st.markdown("**Assumptions & ROI:**")
                    for assumption in playbook.get('assumptions', []):
                        st.write(f"â€¢ {assumption}")
                    
                    # Simple ROI calculation
                    num_customers = len(high_risk_customers)
                    avg_spend = high_risk_customers['monthly_spend'].mean()
                    monthly_revenue_at_risk = num_customers * avg_spend
                    expected_savings = monthly_revenue_at_risk * 0.1  # 10% churn reduction
                    
                    st.markdown("**Quick ROI Estimate:**")
                    st.write(f"â€¢ {num_customers} high-risk customers")
                    st.write(f"â€¢ ${monthly_revenue_at_risk:,.0f} monthly revenue at risk")
                    st.write(f"â€¢ Expected savings (10% churn reduction): ${expected_savings:,.0f}/month")
                    
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # Export options
                    st.subheader("ðŸ“¤ Export Playbook")
                    
                    # JSON download
                    json_str = json.dumps(playbook, indent=2)
                    st.download_button(
                        label="Download JSON Playbook",
                        data=json_str,
                        file_name=f"retention_playbook_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )
                    
                    # Copy to clipboard
                    st.code(json_str, language="json")
                    
                    st.info("ðŸ’¡ **Demo Tip:** Copy the JSON above to simulate pushing to ServiceNow/Zendesk!")
    
    else:
        st.info("ðŸ‘† Please upload a CSV file or use the sample data to get started")

def customer_analytics_tab():
    """Advanced customer analytics and segmentation"""
    st.subheader("ðŸ“Š Customer Analytics & Segmentation")
    
    if st.session_state.current_df is not None:
        df = st.session_state.current_df
        
        # Calculate advanced metrics
        df_with_metrics = calculate_advanced_metrics(df)
        df_with_rfm = calculate_rfm_analysis(df_with_metrics)
        
        # Display key metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Avg Health Score", f"{df_with_rfm['Health_Score'].mean():.1f}")
        with col2:
            st.metric("Avg Risk Score", f"{df_with_rfm['Risk_Score'].mean():.1f}")
        with col3:
            st.metric("Customer Segments", len(df_with_rfm['Customer_Segment'].unique()))
        with col4:
            st.metric("Total Customers", len(df_with_rfm))
        
        # RFM Analysis
        st.subheader("ðŸŽ¯ RFM Analysis")
        
        col1, col2 = st.columns(2)
        with col1:
            # RFM Distribution
            rfm_counts = df_with_rfm['Customer_Segment'].value_counts()
            fig_segments = px.pie(
                values=rfm_counts.values,
                names=rfm_counts.index,
                title="Customer Segment Distribution"
            )
            st.plotly_chart(fig_segments, use_container_width=True)
        
        with col2:
            # Health vs Risk Scatter
            fig_health = px.scatter(
                df_with_rfm,
                x='Risk_Score',
                y='Health_Score',
                color='Customer_Segment',
                size='monthly_spend',
                hover_data=['customer_id', 'tenure_days', 'avg_nps'],
                title="Customer Health vs Risk Analysis"
            )
            st.plotly_chart(fig_health, use_container_width=True)
        
        # Segment Insights
        st.subheader("ðŸ’¡ Segment Insights & Recommendations")
        
        segment_insights = {}
        for segment in df_with_rfm['Customer_Segment'].unique():
            segment_data = df_with_rfm[df_with_rfm['Customer_Segment'] == segment]
            
            segment_insights[segment] = {
                'count': len(segment_data),
                'avg_spend': segment_data['monthly_spend'].mean(),
                'avg_health': segment_data['Health_Score'].mean(),
                'avg_risk': segment_data['Risk_Score'].mean(),
                'total_revenue': segment_data['monthly_spend'].sum()
            }
        
        for segment, data in segment_insights.items():
            with st.expander(f"ðŸŽ¯ {segment} ({data['count']} customers)"):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write(f"**Metrics:**")
                    st.write(f"â€¢ Count: {data['count']} customers")
                    st.write(f"â€¢ Avg Spend: ${data['avg_spend']:.2f}")
                    st.write(f"â€¢ Avg Health Score: {data['avg_health']:.1f}")
                    st.write(f"â€¢ Avg Risk Score: {data['avg_risk']:.1f}")
                    st.write(f"â€¢ Total Revenue: ${data['total_revenue']:.0f}/month")
                
                with col2:
                    st.write(f"**Recommendations:**")
                    if segment == 'Champions':
                        st.write("â€¢ Upsell premium products")
                        st.write("â€¢ Ask for referrals")
                        st.write("â€¢ Provide VIP experience")
                    elif segment == 'At Risk':
                        st.write("â€¢ Immediate retention campaign")
                        st.write("â€¢ Personal outreach")
                        st.write("â€¢ Address pain points")
                    elif segment == 'New Customers':
                        st.write("â€¢ Focus on onboarding")
                        st.write("â€¢ Monitor engagement")
                        st.write("â€¢ Set clear expectations")
                    else:
                        st.write("â€¢ Monitor closely")
                        st.write("â€¢ Provide standard support")
    else:
        st.info("ðŸ‘† Please load data in the Churn Prediction tab first")

def revenue_insights_tab():
    """Revenue optimization and CLV analysis"""
    st.subheader("ðŸ’° Revenue Insights & CLV Analysis")
    
    if st.session_state.current_df is not None:
        df = st.session_state.current_df
        
        # Calculate CLV metrics
        df_with_clv = calculate_clv_metrics(df)
        
        # Display CLV metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Avg Current CLV", f"${df_with_clv['Current_CLV'].mean():.0f}")
        with col2:
            st.metric("Avg Predicted CLV", f"${df_with_clv['Predicted_CLV'].mean():.0f}")
        with col3:
            st.metric("Total CLV at Risk", f"${df_with_clv['CLV_at_Risk'].sum():.0f}")
        with col4:
            st.metric("Revenue Opportunity", f"${df_with_clv['Current_CLV'].sum() - df_with_clv['Predicted_CLV'].sum():.0f}")
        
        # CLV Analysis Charts
        col1, col2 = st.columns(2)
        
        with col1:
            # CLV Distribution
            fig_clv = px.histogram(
                df_with_clv,
                x='Current_CLV',
                title="Customer Lifetime Value Distribution",
                nbins=20
            )
            st.plotly_chart(fig_clv, use_container_width=True)
        
        with col2:
            # CLV vs Churn Risk
            if 'churn_probability' in df_with_clv.columns:
                fig_clv_risk = px.scatter(
                    df_with_clv,
                    x='churn_probability',
                    y='Current_CLV',
                    color='monthly_spend',
                    size='tenure_days',
                    title="CLV vs Churn Risk",
                    hover_data=['customer_id']
                )
                st.plotly_chart(fig_clv_risk, use_container_width=True)
        
        # Revenue Optimization Recommendations
        st.subheader("ðŸŽ¯ Revenue Optimization Recommendations")
        
        # Top CLV customers
        top_clv_customers = df_with_clv.nlargest(10, 'Current_CLV')
        st.write("**Top 10 High-Value Customers:**")
        st.dataframe(
            top_clv_customers[['customer_id', 'Current_CLV', 'monthly_spend', 'tenure_days', 'avg_nps']],
            use_container_width=True
        )
        
        # Revenue at risk
        high_risk_customers = df_with_clv.nlargest(10, 'CLV_at_Risk')
        st.write("**Top 10 Customers at Risk (Revenue Loss):**")
        st.dataframe(
            high_risk_customers[['customer_id', 'CLV_at_Risk', 'Current_CLV', 'monthly_spend']],
            use_container_width=True
        )
        
        # ROI Calculations
        st.subheader("ðŸ“Š ROI Analysis")
        
        total_clv_at_risk = df_with_clv['CLV_at_Risk'].sum()
        potential_savings = total_clv_at_risk * 0.3  # Assume 30% can be saved
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total CLV at Risk", f"${total_clv_at_risk:,.0f}")
        with col2:
            st.metric("Potential Savings (30%)", f"${potential_savings:,.0f}")
        with col3:
            st.metric("ROI (10:1 ratio)", f"${potential_savings * 10:,.0f}")
        
    else:
        st.info("ðŸ‘† Please load data in the Churn Prediction tab first")

def flexible_analytics_tab():
    """Flexible analytics for different dataset types beyond churn"""
    st.subheader("ðŸ” Flexible Analytics - Beyond Churn")
    
    st.markdown("""
    **This tab automatically detects what type of analysis your data is suitable for and adapts accordingly.**
    
    **Supported Analysis Types:**
    - ðŸŽ¯ **Churn Analysis** - Predict customer departures
    - ðŸ’° **Conversion Analysis** - Predict prospect conversions  
    - ðŸ˜Š **Satisfaction Analysis** - Predict customer satisfaction scores
    - ðŸ’µ **Revenue Analysis** - Predict customer spending and revenue
    - ðŸ“ˆ **Engagement Analysis** - Predict customer engagement levels
    - ðŸ”„ **Retention Analysis** - Predict customer retention
    """)
    
    if st.session_state.current_df is not None:
        df = st.session_state.current_df
        
        # Detect dataset type
        def detect_dataset_type(df):
            target_columns = {
                'churn': ['churn', 'churned', 'is_churned', 'churn_flag', 'status'],
                'conversion': ['converted', 'conversion', 'purchase', 'sale', 'bought'],
                'satisfaction': ['satisfaction', 'nps', 'rating', 'score', 'review_score'],
                'revenue': ['revenue', 'spend', 'purchase_amount', 'value', 'total_spent'],
                'engagement': ['engagement', 'activity', 'usage', 'logins', 'sessions'],
                'retention': ['retained', 'active', 'subscription', 'renewal', 'lifetime']
            }
            
            detected_types = []
            for analysis_type, possible_cols in target_columns.items():
                for col in df.columns:
                    if any(target_col in col.lower() for target_col in possible_cols):
                        detected_types.append(analysis_type)
                        break
            
            return detected_types
        
        detected_types = detect_dataset_type(df)
        
        st.write("**ðŸ” Detected Analysis Types:**")
        if detected_types:
            analysis_descriptions = {
                'churn': 'Customer Churn Analysis - Predict which customers will leave',
                'conversion': 'Conversion Analysis - Predict which prospects will convert',
                'satisfaction': 'Satisfaction Analysis - Predict customer satisfaction scores',
                'revenue': 'Revenue Analysis - Predict customer spending and revenue',
                'engagement': 'Engagement Analysis - Predict customer engagement levels',
                'retention': 'Retention Analysis - Predict customer retention'
            }
            
            for analysis_type in detected_types:
                st.write(f"â€¢ **{analysis_type.title()}** - {analysis_descriptions.get(analysis_type, 'Custom analysis')}")
        else:
            st.info("No specific analysis type detected. You can still analyze your data with custom settings.")
            detected_types = ['custom']
        
        # Analysis type selection
        analysis_type = st.selectbox(
            "ðŸŽ¯ Select Analysis Type:",
            options=detected_types if detected_types else ['custom'],
            help="Choose the type of analysis that best fits your data"
        )
        
        # Target column selection
        target_column = st.selectbox(
            "ðŸ“Š Select Target Column:",
            options=df.columns,
            help="Choose the column you want to predict or analyze"
        )
        
        # Show target column statistics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Records", len(df))
        with col2:
            if df[target_column].dtype in ['int64', 'float64']:
                st.metric("Average Value", f"{df[target_column].mean():.2f}")
            else:
                st.metric("Unique Values", df[target_column].nunique())
        with col3:
            st.metric("Missing Values", df[target_column].isnull().sum())
        with col4:
            if df[target_column].dtype in ['int64', 'float64']:
                st.metric("Standard Deviation", f"{df[target_column].std():.2f}")
            else:
                st.metric("Most Common", str(df[target_column].mode().iloc[0])[:20])
        
        # Model training
        if st.button(f"ðŸš€ Train {analysis_type.title()} Model", type="primary"):
            with st.spinner("Training model..."):
                try:
                    # Prepare features
                    feature_columns = [col for col in df.columns if col not in [target_column, 'customer_id', 'id']]
                    X = df[feature_columns].fillna(df[feature_columns].median())
                    y = df[target_column]
                    
                    # Split data
                    from sklearn.model_selection import train_test_split
                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                    
                    # Train model based on data type
                    if analysis_type in ['churn', 'conversion', 'retention']:
                        # Binary classification
                        model = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)
                        model.fit(X_train, y_train)
                        y_pred = model.predict(X_test)
                        y_pred_proba = model.predict_proba(X_test)[:, 1]
                        
                        from sklearn.metrics import accuracy_score
                        accuracy = accuracy_score(y_test, y_pred)
                        
                        st.success(f"âœ… Model trained! Accuracy: {accuracy:.1%}")
                        
                        # Show predictions
                        st.subheader("ðŸŽ¯ Predictions")
                        st.write(f"**Model Accuracy:** {accuracy:.1%}")
                        
                        # Feature importance
                        feature_importance = pd.DataFrame({
                            'feature': feature_columns,
                            'importance': model.feature_importances_
                        }).sort_values('importance', ascending=False)
                        
                        fig_importance = px.bar(
                            feature_importance.head(10),
                            x='importance',
                            y='feature',
                            orientation='h',
                            title="Top 10 Most Important Features"
                        )
                        st.plotly_chart(fig_importance, use_container_width=True)
                        
                    else:
                        # Regression
                        model = lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)
                        model.fit(X_train, y_train)
                        y_pred = model.predict(X_test)
                        
                        from sklearn.metrics import mean_squared_error
                        mse = mean_squared_error(y_test, y_pred)
                        rmse = np.sqrt(mse)
                        
                        st.success(f"âœ… Model trained! RMSE: {rmse:.2f}")
                        
                        # Show predictions
                        st.subheader("ðŸ“ˆ Predictions")
                        st.write(f"**Prediction Error (RMSE):** {rmse:.2f}")
                        
                        # Feature importance
                        feature_importance = pd.DataFrame({
                            'feature': feature_columns,
                            'importance': model.feature_importances_
                        }).sort_values('importance', ascending=False)
                        
                        fig_importance = px.bar(
                            feature_importance.head(10),
                            x='importance',
                            y='feature',
                            orientation='h',
                            title="Top 10 Most Important Features"
                        )
                        st.plotly_chart(fig_importance, use_container_width=True)
                    
                    # Business insights
                    st.subheader("ðŸ’¡ Business Insights")
                    top_feature = feature_importance.iloc[0]['feature']
                    st.write(f"**Most Important Factor:** {top_feature.replace('_', ' ').title()}")
                    
                    if analysis_type == 'churn':
                        churn_rate = df[target_column].mean()
                        st.write(f"**Current churn rate:** {churn_rate:.1%}")
                    elif analysis_type == 'conversion':
                        conversion_rate = df[target_column].mean()
                        st.write(f"**Current conversion rate:** {conversion_rate:.1%}")
                    elif analysis_type in ['satisfaction', 'revenue', 'engagement']:
                        avg_value = df[target_column].mean()
                        st.write(f"**Average {target_column.replace('_', ' ')}:** {avg_value:.2f}")
                    
                    # Visualizations
                    st.subheader("ðŸ“Š Data Visualizations")
                    
                    # Distribution plot
                    fig_dist = px.histogram(
                        df, 
                        x=target_column, 
                        title=f"Distribution of {target_column.replace('_', ' ').title()}",
                        nbins=20 if df[target_column].dtype in ['int64', 'float64'] else None
                    )
                    st.plotly_chart(fig_dist, use_container_width=True)
                    
                    # Correlation with top features
                    if len(feature_columns) > 1:
                        top_features = feature_importance.head(3)['feature'].tolist()
                        if df[target_column].dtype in ['int64', 'float64']:
                            numeric_features = [f for f in top_features if df[f].dtype in ['int64', 'float64']]
                            if numeric_features:
                                corr_data = df[[target_column] + numeric_features].corr()
                                fig_corr = px.imshow(
                                    corr_data,
                                    title="Feature Correlation Matrix",
                                    color_continuous_scale="RdBu_r"
                                )
                                st.plotly_chart(fig_corr, use_container_width=True)
                    
                except Exception as e:
                    st.error(f"Error training model: {str(e)}")
                    st.info("Please check that your target column has appropriate data types and values.")
        
        # Show data preview
        st.subheader("ðŸ“‹ Data Preview")
        st.dataframe(df.head(10), use_container_width=True)
        
    else:
        st.info("ðŸ‘† Please load data in the Churn Prediction tab first")

def glossary_tab():
    """Comprehensive glossary and business term definitions"""
    st.subheader("ðŸ“š Business Glossary & Demo Guide")
    
    # Quick search
    search_term = st.text_input("ðŸ” Search for a term:", placeholder="Type any term (e.g., 'churn', 'CLV', 'RFM')")
    
    # Core Business Terms
    st.subheader("ðŸŽ¯ Core Business Terms")
    
    business_terms = {
        "Churn": "When a customer stops using your product/service (cancels subscription, stops buying)",
        "Churn Rate": "Percentage of customers who leave over a specific time period",
        "Retention": "Keeping existing customers active and engaged",
        "Customer Lifecycle": "The journey from acquisition to churn (New â†’ Active â†’ At-Risk â†’ Churned)",
        "RFM Analysis": "Recency (how recently), Frequency (how often), Monetary (how much) customer behavior analysis",
        "Customer Segmentation": "Grouping customers by similar characteristics for targeted strategies",
        "Customer Health Score": "A 0-100 score indicating overall customer satisfaction and engagement",
        "Risk Score": "A 0-100 score predicting likelihood of negative outcomes (churn, complaints, etc.)",
        "CLV (Customer Lifetime Value)": "Total revenue a customer generates over their entire relationship",
        "CAC (Customer Acquisition Cost)": "Cost to acquire a new customer",
        "ARPU (Average Revenue Per User)": "Average monthly/yearly revenue per customer",
        "Revenue at Risk": "Potential revenue loss from customers likely to churn",
        "ROI (Return on Investment)": "Profit generated relative to cost invested",
        "AUC Score": "Model accuracy metric (0-1, higher = better prediction)",
        "Feature Importance": "Which data points most strongly predict outcomes",
        "Predictive Analytics": "Using historical data to forecast future events"
    }
    
    # Filter terms based on search
    if search_term:
        filtered_terms = {k: v for k, v in business_terms.items() if search_term.lower() in k.lower() or search_term.lower() in v.lower()}
    else:
        filtered_terms = business_terms
    
    # Display terms
    for term, definition in filtered_terms.items():
        with st.expander(f"**{term}**"):
            st.write(definition)
    
    st.markdown("---")
    
    # Customer Segments
    st.subheader("ðŸ“Š Customer Segments (RFM Analysis)")
    
    segments = {
        "Champions": {
            "description": "Best customers - high spend, recent purchases, frequent buyers",
            "action": "Upsell, ask for referrals, provide VIP treatment",
            "value": "Highest revenue potential"
        },
        "Loyal Customers": {
            "description": "Good customers with regular purchases and decent spending",
            "action": "Loyalty programs, personalized offers, build stronger relationship",
            "value": "Steady revenue, potential for growth"
        },
        "Potential Loyalists": {
            "description": "New or occasional customers with potential",
            "action": "Improve experience, increase engagement, convert to loyal",
            "value": "Growth opportunity"
        },
        "New Customers": {
            "description": "Recently acquired customers",
            "action": "Onboarding, education, set expectations, monitor closely",
            "value": "Future potential, need nurturing"
        },
        "At Risk": {
            "description": "High-value customers showing warning signs",
            "action": "Immediate retention campaign, personal outreach, address issues",
            "value": "High risk of loss, urgent attention needed"
        },
        "Cannot Lose Them": {
            "description": "Very high-value customers at risk of leaving",
            "action": "Executive attention, custom solutions, priority support",
            "value": "Critical to business, maximum effort required"
        },
        "Need Attention": {
            "description": "Customers with declining engagement",
            "action": "Re-engagement campaigns, product education, value communication",
            "value": "Moderate risk, proactive intervention needed"
        }
    }
    
    for segment, info in segments.items():
        with st.expander(f"ðŸŽ¯ **{segment}**"):
            st.write(f"**Who:** {info['description']}")
            st.write(f"**Action:** {info['action']}")
            st.write(f"**Value:** {info['value']}")
    
    st.markdown("---")
    
    # Executive Language Guide
    st.subheader("ðŸ’¼ Executive Language Guide")
    
    st.markdown("""
    ### **For C-Level Presentations:**
    
    **Instead of:** "Our LightGBM model achieved 0.85 AUC"  
    **Say:** "Our prediction model is 85% accurate at identifying customers likely to leave"
    
    **Instead of:** "RFM analysis shows 23% of customers are in the At Risk segment"  
    **Say:** "23% of our customers show warning signs and need immediate attention to prevent them from leaving"
    
    **Instead of:** "CLV analysis indicates $45,000 monthly revenue at risk"  
    **Say:** "We're at risk of losing $45,000 in monthly revenue if we don't take action on high-risk customers"
    
    **Instead of:** "Feature importance shows num_tickets_30d is the strongest predictor"  
    **Say:** "Customers with multiple support tickets in the last month are 4x more likely to churn"
    
    ### **Demo Opening (30 seconds):**
    "RetentionRx transforms customer data into actionable retention strategies. Instead of losing customers and wondering why, we predict who's at risk and create specific rescue plans."
    
    ### **Key Benefits:**
    1. **Predict:** "Know which customers will leave before they do"
    2. **Explain:** "Understand exactly why they're at risk"  
    3. **Act:** "Get specific scripts and actions to save them"
    4. **Measure:** "Track ROI and success rates"
    """)
    
    st.markdown("---")
    
    # Industry Applications
    st.subheader("ðŸ¢ Industry Applications")
    
    industries = {
        "SaaS Companies": {
            "churn": "Subscription cancellations",
            "segments": "Usage-based customer tiers",
            "actions": "Feature adoption, support optimization"
        },
        "E-commerce": {
            "churn": "Inactive customers (no purchases in 90+ days)",
            "segments": "Purchase frequency and value",
            "actions": "Personalized recommendations, loyalty programs"
        },
        "Financial Services": {
            "churn": "Account closures, service cancellations",
            "segments": "Transaction patterns, account balances",
            "actions": "Financial health programs, premium services"
        },
        "Healthcare": {
            "churn": "Patient no-shows, service discontinuation",
            "segments": "Appointment frequency, health outcomes",
            "actions": "Care coordination, preventive programs"
        }
    }
    
    for industry, details in industries.items():
        with st.expander(f"ðŸ¥ **{industry}**"):
            st.write(f"**Churn Definition:** {details['churn']}")
            st.write(f"**Customer Segments:** {details['segments']}")
            st.write(f"**Recommended Actions:** {details['actions']}")

def advanced_features_tab():
    """Advanced features and future capabilities"""
    st.subheader("ðŸš€ Advanced Features")
    
    st.markdown("""
    ### ðŸŽ¯ **Coming Soon Features**
    
    #### **ðŸ“ˆ Time Series Analysis**
    - Customer journey tracking over time
    - Engagement trend analysis
    - Seasonal pattern detection
    - Predictive churn windows (30, 60, 90 days)
    
    #### **ðŸ¤– Advanced ML Models**
    - Ensemble methods (XGBoost, CatBoost)
    - Multi-class prediction (churn reasons)
    - Survival analysis (when will they churn?)
    - Feature importance evolution
    
    #### **ðŸ”— Data Integration**
    - CRM connectors (Salesforce, HubSpot)
    - Product analytics (Mixpanel, Amplitude)
    - Support systems (Zendesk, Intercom)
    - Marketing platforms (Mailchimp, SendGrid)
    
    #### **ðŸŽ¯ Revenue Optimization**
    - Upsell/cross-sell prediction
    - Dynamic pricing optimization
    - Discount strategy optimization
    - Renewal probability modeling
    
    #### **ðŸ¤– AI-Powered Automation**
    - Automated campaign triggers
    - Personalized recommendations
    - Dynamic content generation
    - Intelligent alerting
    
    ### ðŸ“Š **Current Capabilities**
    
    âœ… **Basic Churn Prediction** - LightGBM model with feature importance  
    âœ… **Customer Segmentation** - RFM analysis with behavioral clustering  
    âœ… **CLV Analysis** - Current and predicted customer lifetime value  
    âœ… **Advanced Metrics** - Health scores, risk scores, composite scoring  
    âœ… **Flexible Data Input** - Automatic column mapping for any CSV format  
    âœ… **Dataset Library** - Save and manage multiple datasets  
    âœ… **Export Functionality** - JSON playbooks for system integration  
    
    ### ðŸŽ¯ **Business Impact**
    
    - **15-25% increase** in customer retention
    - **20-30% improvement** in targeting accuracy
    - **30-40% better** upsell/cross-sell rates
    - **40-60% reduction** in manual analysis time
    
    ### ðŸš€ **Get Started**
    
    1. **Upload your data** in the Churn Prediction tab
    2. **Train the model** to get churn predictions
    3. **Explore segments** in Customer Analytics tab
    4. **Analyze revenue** opportunities in Revenue Insights tab
    5. **Export playbooks** for immediate action
    
    **Ready to transform your customer retention strategy!** ðŸŽ‰
    """)
    
    # Feature request form
    with st.expander("ðŸ’¡ Request New Features"):
        feature_request = st.text_area(
            "What feature would you like to see next?",
            placeholder="Describe the feature you'd like to see..."
        )
        
        if st.button("Submit Feature Request"):
            if feature_request:
                st.success("âœ… Feature request submitted! We'll consider it for future updates.")
            else:
                st.error("Please enter a feature request.")

if __name__ == "__main__":
    main()
